{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joska\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joska\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# # Original losses:\n",
    "# # -------------------\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative losses:\n",
    "# # -------------------\n",
    "# D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "# D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "# D_loss = D_loss_real + D_loss_fake\n",
    "# G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN8UlEQVR4nO3dfahc9Z3H8c/HtI3gFZI0GGLqrrWoJCnELiEGV5cukpr1Hy1IqMrqutL4h0EFEcX9w6islmV1EQOFW3xITdcg+JTUYnVDWV2QksTHaNb6EGMS8rAhoAmi9Sbf/eOeyK3e+c3NzJk5k/t9v+AyM+c7Z86XQz45T3Pm54gQgMnvhKYbANAfhB1IgrADSRB2IAnCDiTxrX4uzDan/oEeiwiPN72rLbvtpbbftf2+7du6+SwAveVOr7PbniLpT5KWSNopaaOkyyPincI8bNmBHuvFln2RpPcj4sOI+LOktZIu6eLzAPRQN2GfI2nHmNc7q2l/wfZy25tsb+piWQC61PMTdBExLGlYYjceaFI3W/Zdkk4b8/p71TQAA6ibsG+UdKbt79v+jqSfSVpXT1sA6tbxbnxEjNheIen3kqZIejgi3q6tMwC16vjSW0cL45gd6LmefKkGwPGDsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ6HrIZx4cpU6YU69OnT+/p8leuXNmyNjQ0VJx33rx5xfpll11WrK9Zs6Zl7YILLijOOzIyUqwPDw8X69dff32x3oSuwm77I0kHJR2WNBIRC+toCkD96tiy/31E7K/hcwD0EMfsQBLdhj0kvWB7s+3l473B9nLbm2xv6nJZALrQ7W78+RGxy/Ypkl60/b8R8dLYN0TEsKRhSbIdXS4PQIe62rJHxK7qcZ+kpyUtqqMpAPXrOOy2T7J98tHnkn4iaUtdjQGoVze78bMkPW376Of8Z0Q8X0tXk8wZZ5xRrJ944onF+kUXXVSsL1mypGVt2rRpxXkXL15crDfp008/LdafeOKJYn3RotY7ml988UVx3h07dhTrGzZsKNYHUcdhj4gPJS2osRcAPcSlNyAJwg4kQdiBJAg7kARhB5JwRP++1DZZv0HX7nbJF154oVifOnVqne0cN9r927v55puL9UOHDnW87HaX1vbs2VOsv/HGGx0vu9ciwuNNZ8sOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnb0GM2fOLNbffffdYr3XP+fcjW3bthXrBw8eLNbnz5/fsnb48OHivO1u/cX4uM4OJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwZHMN9u8vj2t5yy23FOvLli0r1l955ZVi/Y477ijWS3bu3FmsL1hQ/gHhdveUL1zYemDfu+66qzgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72AdBuWOVPPvmkWH/uueda1pYuXVqc98YbbyzWH3zwwWIdg6fj+9ltP2x7n+0tY6bNsP2i7feqx8H99QUAkia2G/+opK9vHm6TtCEizpS0oXoNYIC1DXtEvCTpwNcmXyJpdfV8taRL620LQN06/W78rIjYXT3fI2lWqzfaXi5peYfLAVCTrm+EiYgonXiLiGFJwxIn6IAmdXrpba/t2ZJUPe6rryUAvdBp2NdJurp6frWkZ+tpB0CvtL3ObvtxST+WNFPSXkl3SHpG0hOS/krSdknLIuLrJ/HG+yx243tgzZo1LWtXXHFFcd52v2lf+t13STpy5Eixjv5rdZ297TF7RFzeonRhVx0B6Cu+LgskQdiBJAg7kARhB5Ig7EAS3OI6CQwNDbWsbdy4sTjv2WefXay3u3S3du3aYh39x5DNQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE19knublz5xbrr732WrH++eefF+ubN28u1l9++eWWtTvvvLM4bz//bU4mXGcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4zp7ctddeW6yvWrWqWJ86dWrHy77//vuL9QceeKBY37FjR8fLnsy4zg4kR9iBJAg7kARhB5Ig7EAShB1IgrADSXCdHUXnnntusf7QQw8V6/Pmzet42evXry/Wb7jhhmJ9+/btHS/7eNbxdXbbD9veZ3vLmGkrbe+y/Xr1d3GdzQKo30R24x+VtHSc6f8REedUf7+rty0AdWsb9oh4SdKBPvQCoIe6OUG3wvab1W7+9FZvsr3c9ibbm7pYFoAudRr2X0r6gaRzJO2WdF+rN0bEcEQsjIiFHS4LQA06CntE7I2IwxFxRNKvJC2qty0Adeso7LZnj3n5U0lbWr0XwGBoe53d9uOSfixppqS9ku6oXp8jKSR9JOm6iNjddmFcZ590ZsyYUaxfddVVLWv33dfy6E+SZI97ufgrW7duLdbnz59frE9Wra6zf2sCM14+zuTyNykADBy+LgskQdiBJAg7kARhB5Ig7EAS3OKKxoyMjBTrJ5xQ3hYdOXKkWF+2bFnL2lNPPVWc93jGT0kDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJt73pDbosXLy7Wr7nmmo7nb3cdvZ09e/YU688880xXnz/ZsGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS4zj7JLViwoFhfuXJlsX7hhRcW60NDQ8fa0oS1u199//79Xc2fDVt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC6+zHgTlz5hTrK1asaFm77rrrivNOmzatk5Zq8fHHHxfr7b4D8Oijj9bXTAJtt+y2T7P9B9vv2H7b9o3V9Bm2X7T9XvU4vfftAujURHbjRyTdHBHzJC2WdL3teZJuk7QhIs6UtKF6DWBAtQ17ROyOiFer5wclbZU0R9IlklZXb1st6dIe9QigBsd0zG77dEk/kvRHSbMiYndV2iNpVot5lkta3kWPAGow4bPxtockPSnppoj4dGwtRkeHHHfQxogYjoiFEbGwq04BdGVCYbf9bY0G/TcRcXT4y722Z1f12ZL29aZFAHVouxtv25IekrQ1Iu4fU1on6WpJv6gen+1Jh5PAqaeeWqyfd955xfqqVauK9VNOOeWYe6rLtm3bivV77rmnZe2RRx4pzsstqvWayDH730r6R0lv2X69mna7RkP+hO1rJW2X1HowbACNaxv2iPgfSeMO7i6p/MsGAAYGX5cFkiDsQBKEHUiCsANJEHYgCW5xnaCZM2e2rK1fv74471lnnVWsT5/e3A2DH3zwQbF+7733Futr164t1j/77LNj7gm9wZYdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JIc519yZIlxfrdd99drM+dO7dl7eSTT+6op7p8+eWXLWuPPfZYcd6bbrqpWD906FAnLWEAsWUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTSXGe/8sori/VFixb1bNl79+4t1p9//vlifWRkpFi/9dZbW9YOHDhQnBd5sGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQcEeU32KdJ+rWkWZJC0nBEPGB7paSfS/q/6q23R8Tv2nxWeWEAuhYR4466PJGwz5Y0OyJetX2ypM2SLtXoeOyHIuLfJ9oEYQd6r1XYJzI++25Ju6vnB21vlTSn3vYA9NoxHbPbPl3SjyT9sZq0wvabth+2Pe4YRraX295ke1N3rQLoRtvd+K/eaA9J+m9J/xoRT9meJWm/Ro/j79borv4/t/kMduOBHuv4mF2SbH9b0m8l/T4i7h+nfrqk30bED9t8DmEHeqxV2Nvuxtu2pIckbR0b9OrE3VE/lbSl2yYB9M5EzsafL+llSW9JOlJNvl3S5ZLO0ehu/EeSrqtO5pU+iy070GNd7cbXhbADvdfxbjyAyYGwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRL+HbN4vafuY1zOraYNoUHsb1L4keutUnb39datCX+9n/8bC7U0RsbCxBgoGtbdB7Uuit071qzd244EkCDuQRNNhH254+SWD2tug9iXRW6f60lujx+wA+qfpLTuAPiHsQBKNhN32Utvv2n7f9m1N9NCK7Y9sv2X79abHp6vG0Ntne8uYaTNsv2j7vepx3DH2Guptpe1d1bp73fbFDfV2mu0/2H7H9tu2b6ymN7ruCn31Zb31/Zjd9hRJf5K0RNJOSRslXR4R7/S1kRZsfyRpYUQ0/gUM238n6ZCkXx8dWsv2v0k6EBG/qP6jnB4Rtw5Ibyt1jMN496i3VsOM/5MaXHd1Dn/eiSa27IskvR8RH0bEnyWtlXRJA30MvIh4SdKBr02+RNLq6vlqjf5j6bsWvQ2EiNgdEa9Wzw9KOjrMeKPrrtBXXzQR9jmSdox5vVODNd57SHrB9mbby5tuZhyzxgyztUfSrCabGUfbYbz76WvDjA/Muutk+PNucYLum86PiL+R9A+Srq92VwdSjB6DDdK1019K+oFGxwDcLem+Jpuphhl/UtJNEfHp2FqT626cvvqy3poI+y5Jp415/b1q2kCIiF3V4z5JT2v0sGOQ7D06gm71uK/hfr4SEXsj4nBEHJH0KzW47qphxp+U9JuIeKqa3Pi6G6+vfq23JsK+UdKZtr9v+zuSfiZpXQN9fIPtk6oTJ7J9kqSfaPCGol4n6erq+dWSnm2wl78wKMN4txpmXA2vu8aHP4+Ivv9JulijZ+Q/kPQvTfTQoq8zJL1R/b3ddG+SHtfobt2XGj23ca2k70raIOk9Sf8lacYA9faYRof2flOjwZrdUG/na3QX/U1Jr1d/Fze97gp99WW98XVZIAlO0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8PeyZ6ORV+ka8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import MNISTDataset\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
    "\n",
    "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
    "                    test_images.reshape([-1, 784]), test_labels,\n",
    "                    batch_size=128)\n",
    "\n",
    "                    \n",
    "for it in range(5000):\n",
    "    X_mb,_ = data.next_batch()\n",
    "    if X_mb.shape == (96,784):\n",
    "        print(X_mb.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "    mb_size = 128\n",
    "    Z_dim = 100\n",
    "    n_iterations = 100000\n",
    "    path = \"out_org_100k/\"\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for it in range(n_iterations+1):\n",
    "        if it % 1000 == 0:\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out_org_100k/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)\n",
    "\n",
    "        X_mb, _ = data.next_batch()\n",
    "\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "        if math.isnan(D_loss_curr):\n",
    "            print(\"oh no loss became nan at iteration\",it) \n",
    "            return False\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter: {}'.format(it))\n",
    "            print('D loss: {:.4}'. format(D_loss_curr))\n",
    "            print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "            print()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D loss: 1.442\n",
      "G_loss: 2.482\n",
      "\n",
      "Iter: 1000\n",
      "D loss: 0.005127\n",
      "G_loss: 7.26\n",
      "\n",
      "Iter: 2000\n",
      "D loss: 0.06981\n",
      "G_loss: 5.312\n",
      "\n",
      "Iter: 3000\n",
      "D loss: 0.04599\n",
      "G_loss: 5.781\n",
      "\n",
      "Iter: 4000\n",
      "D loss: 0.1338\n",
      "G_loss: 5.184\n",
      "\n",
      "Iter: 5000\n",
      "D loss: 0.2665\n",
      "G_loss: 4.092\n",
      "\n",
      "Iter: 6000\n",
      "D loss: 0.339\n",
      "G_loss: 3.934\n",
      "\n",
      "Iter: 7000\n",
      "D loss: 0.548\n",
      "G_loss: 4.381\n",
      "\n",
      "Iter: 8000\n",
      "D loss: 0.568\n",
      "G_loss: 3.316\n",
      "\n",
      "Iter: 9000\n",
      "D loss: 0.7596\n",
      "G_loss: 2.849\n",
      "\n",
      "Iter: 10000\n",
      "D loss: 0.9443\n",
      "G_loss: 1.866\n",
      "\n",
      "Iter: 11000\n",
      "D loss: 0.6117\n",
      "G_loss: 2.414\n",
      "\n",
      "Iter: 12000\n",
      "D loss: 0.8868\n",
      "G_loss: 1.994\n",
      "\n",
      "Iter: 13000\n",
      "D loss: 0.7106\n",
      "G_loss: 2.936\n",
      "\n",
      "Iter: 14000\n",
      "D loss: 0.6912\n",
      "G_loss: 2.546\n",
      "\n",
      "Iter: 15000\n",
      "D loss: 0.7281\n",
      "G_loss: 2.38\n",
      "\n",
      "Iter: 16000\n",
      "D loss: 0.6598\n",
      "G_loss: 2.147\n",
      "\n",
      "Iter: 17000\n",
      "D loss: 0.8806\n",
      "G_loss: 2.188\n",
      "\n",
      "Iter: 18000\n",
      "D loss: 0.6018\n",
      "G_loss: 2.188\n",
      "\n",
      "Iter: 19000\n",
      "D loss: 0.8143\n",
      "G_loss: 1.832\n",
      "\n",
      "Iter: 20000\n",
      "D loss: 0.7984\n",
      "G_loss: 1.973\n",
      "\n",
      "Iter: 21000\n",
      "D loss: 0.9113\n",
      "G_loss: 2.134\n",
      "\n",
      "Iter: 22000\n",
      "D loss: 0.6434\n",
      "G_loss: 1.963\n",
      "\n",
      "Iter: 23000\n",
      "D loss: 0.8628\n",
      "G_loss: 2.241\n",
      "\n",
      "Iter: 24000\n",
      "D loss: 0.9799\n",
      "G_loss: 1.894\n",
      "\n",
      "Iter: 25000\n",
      "D loss: 0.982\n",
      "G_loss: 1.773\n",
      "\n",
      "Iter: 26000\n",
      "D loss: 0.8134\n",
      "G_loss: 1.801\n",
      "\n",
      "Iter: 27000\n",
      "D loss: 0.6555\n",
      "G_loss: 2.093\n",
      "\n",
      "Iter: 28000\n",
      "D loss: 0.6725\n",
      "G_loss: 2.027\n",
      "\n",
      "Iter: 29000\n",
      "D loss: 0.9683\n",
      "G_loss: 1.829\n",
      "\n",
      "Iter: 30000\n",
      "D loss: 0.8896\n",
      "G_loss: 1.988\n",
      "\n",
      "Iter: 31000\n",
      "D loss: 0.8468\n",
      "G_loss: 1.817\n",
      "\n",
      "Iter: 32000\n",
      "D loss: 0.9546\n",
      "G_loss: 1.517\n",
      "\n",
      "Iter: 33000\n",
      "D loss: 0.9671\n",
      "G_loss: 2.049\n",
      "\n",
      "Iter: 34000\n",
      "D loss: 0.7919\n",
      "G_loss: 2.114\n",
      "\n",
      "Iter: 35000\n",
      "D loss: 0.8902\n",
      "G_loss: 1.903\n",
      "\n",
      "Iter: 36000\n",
      "D loss: 0.9145\n",
      "G_loss: 1.941\n",
      "\n",
      "Iter: 37000\n",
      "D loss: 0.705\n",
      "G_loss: 2.293\n",
      "\n",
      "Iter: 38000\n",
      "D loss: 0.8435\n",
      "G_loss: 1.887\n",
      "\n",
      "Iter: 39000\n",
      "D loss: 0.7478\n",
      "G_loss: 2.134\n",
      "\n",
      "Iter: 40000\n",
      "D loss: 0.7276\n",
      "G_loss: 1.933\n",
      "\n",
      "Iter: 41000\n",
      "D loss: 0.7939\n",
      "G_loss: 1.959\n",
      "\n",
      "Iter: 42000\n",
      "D loss: 0.6339\n",
      "G_loss: 2.2\n",
      "\n",
      "Iter: 43000\n",
      "D loss: 0.9802\n",
      "G_loss: 2.068\n",
      "\n",
      "Iter: 44000\n",
      "D loss: 0.8941\n",
      "G_loss: 1.792\n",
      "\n",
      "Iter: 45000\n",
      "D loss: 0.9023\n",
      "G_loss: 1.828\n",
      "\n",
      "Iter: 46000\n",
      "D loss: 0.8626\n",
      "G_loss: 1.626\n",
      "\n",
      "Iter: 47000\n",
      "D loss: 0.8913\n",
      "G_loss: 1.801\n",
      "\n",
      "Iter: 48000\n",
      "D loss: 0.8391\n",
      "G_loss: 1.932\n",
      "\n",
      "Iter: 49000\n",
      "D loss: 0.8687\n",
      "G_loss: 2.013\n",
      "\n",
      "Iter: 50000\n",
      "D loss: 0.7987\n",
      "G_loss: 2.072\n",
      "\n",
      "Iter: 51000\n",
      "D loss: 0.9437\n",
      "G_loss: 1.83\n",
      "\n",
      "Iter: 52000\n",
      "D loss: 0.8398\n",
      "G_loss: 1.94\n",
      "\n",
      "Iter: 53000\n",
      "D loss: 0.9186\n",
      "G_loss: 1.673\n",
      "\n",
      "Iter: 54000\n",
      "D loss: 0.855\n",
      "G_loss: 1.818\n",
      "\n",
      "Iter: 55000\n",
      "D loss: 0.865\n",
      "G_loss: 1.657\n",
      "\n",
      "Iter: 56000\n",
      "D loss: 0.9393\n",
      "G_loss: 1.804\n",
      "\n",
      "Iter: 57000\n",
      "D loss: 0.6842\n",
      "G_loss: 1.95\n",
      "\n",
      "Iter: 58000\n",
      "D loss: 0.8284\n",
      "G_loss: 1.667\n",
      "\n",
      "Iter: 59000\n",
      "D loss: 0.7008\n",
      "G_loss: 2.152\n",
      "\n",
      "Iter: 60000\n",
      "D loss: 0.7998\n",
      "G_loss: 2.082\n",
      "\n",
      "Iter: 61000\n",
      "D loss: 0.767\n",
      "G_loss: 1.682\n",
      "\n",
      "Iter: 62000\n",
      "D loss: 0.8176\n",
      "G_loss: 1.897\n",
      "\n",
      "Iter: 63000\n",
      "D loss: 0.7593\n",
      "G_loss: 2.193\n",
      "\n",
      "Iter: 64000\n",
      "D loss: 0.7202\n",
      "G_loss: 1.909\n",
      "\n",
      "Iter: 65000\n",
      "D loss: 0.7152\n",
      "G_loss: 2.163\n",
      "\n",
      "Iter: 66000\n",
      "D loss: 0.7854\n",
      "G_loss: 2.106\n",
      "\n",
      "Iter: 67000\n",
      "D loss: 0.6049\n",
      "G_loss: 2.026\n",
      "\n",
      "Iter: 68000\n",
      "D loss: 0.7843\n",
      "G_loss: 1.98\n",
      "\n",
      "Iter: 69000\n",
      "D loss: 0.739\n",
      "G_loss: 1.999\n",
      "\n",
      "Iter: 70000\n",
      "D loss: 0.8177\n",
      "G_loss: 1.979\n",
      "\n",
      "Iter: 71000\n",
      "D loss: 0.7744\n",
      "G_loss: 1.688\n",
      "\n",
      "Iter: 72000\n",
      "D loss: 0.6702\n",
      "G_loss: 2.524\n",
      "\n",
      "Iter: 73000\n",
      "D loss: 0.7468\n",
      "G_loss: 2.339\n",
      "\n",
      "Iter: 74000\n",
      "D loss: 0.7336\n",
      "G_loss: 2.126\n",
      "\n",
      "Iter: 75000\n",
      "D loss: 0.898\n",
      "G_loss: 1.923\n",
      "\n",
      "Iter: 76000\n",
      "D loss: 0.788\n",
      "G_loss: 2.176\n",
      "\n",
      "Iter: 77000\n",
      "D loss: 0.6718\n",
      "G_loss: 1.825\n",
      "\n",
      "Iter: 78000\n",
      "D loss: 0.6941\n",
      "G_loss: 2.408\n",
      "\n",
      "Iter: 79000\n",
      "D loss: 0.7028\n",
      "G_loss: 1.901\n",
      "\n",
      "Iter: 80000\n",
      "D loss: 0.7874\n",
      "G_loss: 2.007\n",
      "\n",
      "Iter: 81000\n",
      "D loss: 0.8544\n",
      "G_loss: 2.107\n",
      "\n",
      "Iter: 82000\n",
      "D loss: 0.6935\n",
      "G_loss: 2.102\n",
      "\n",
      "Iter: 83000\n",
      "D loss: 0.6834\n",
      "G_loss: 2.038\n",
      "\n",
      "Iter: 84000\n",
      "D loss: 0.6331\n",
      "G_loss: 2.186\n",
      "\n",
      "Iter: 85000\n",
      "D loss: 0.599\n",
      "G_loss: 2.037\n",
      "\n",
      "Iter: 86000\n",
      "D loss: 0.6063\n",
      "G_loss: 2.103\n",
      "\n",
      "Iter: 87000\n",
      "D loss: 0.5603\n",
      "G_loss: 2.154\n",
      "\n",
      "Iter: 88000\n",
      "D loss: 0.6829\n",
      "G_loss: 2.419\n",
      "\n",
      "Iter: 89000\n",
      "D loss: 0.7053\n",
      "G_loss: 2.643\n",
      "\n",
      "Iter: 90000\n",
      "D loss: 0.604\n",
      "G_loss: 2.271\n",
      "\n",
      "Iter: 91000\n",
      "D loss: 0.847\n",
      "G_loss: 2.096\n",
      "\n",
      "Iter: 92000\n",
      "D loss: 0.6459\n",
      "G_loss: 2.069\n",
      "\n",
      "Iter: 93000\n",
      "D loss: 0.5894\n",
      "G_loss: 2.167\n",
      "\n",
      "Iter: 94000\n",
      "D loss: 0.6696\n",
      "G_loss: 2.034\n",
      "\n",
      "Iter: 95000\n",
      "D loss: 0.651\n",
      "G_loss: 2.445\n",
      "\n",
      "Iter: 96000\n",
      "D loss: 0.6188\n",
      "G_loss: 2.114\n",
      "\n",
      "Iter: 97000\n",
      "D loss: 0.6275\n",
      "G_loss: 2.293\n",
      "\n",
      "Iter: 98000\n",
      "D loss: 0.5483\n",
      "G_loss: 2.18\n",
      "\n",
      "Iter: 99000\n",
      "D loss: 0.7301\n",
      "G_loss: 2.134\n",
      "\n",
      "Iter: 100000\n",
      "D loss: 0.5284\n",
      "G_loss: 2.612\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "attemps = 0\n",
    "while not train():\n",
    "    attemps+=1\n",
    "\n",
    "print (attemps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffb6baedebf6957ac36c66a06d806fe87b8bfbad768888f61612b8fcf4c95baf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
