{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "import tensorflow_datasets as tfds\n",
    "mnist = tfds.load(name='mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Net\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784], name='X')\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]), name='D_W1')\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]), name='D_b1')\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]), name='D_W2')\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Net\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100], name='Z')\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]), name='G_W1')\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]), name='G_b1')\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]), name='G_W2')\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]), name='G_b2')\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(current_pos, batch_size, end_pos, size, train_data, train_labels):\n",
    "        \"\"\"Either gets the next batch, or optionally shuffles and starts a\n",
    "        new epoch.\"\"\"\n",
    "        end_pos = current_pos + batch_size\n",
    "        if end_pos < size:\n",
    "            batch = (train_data[current_pos:end_pos],\n",
    "                     train_labels[current_pos:end_pos])\n",
    "            current_pos +=batch_size\n",
    "        else:\n",
    "            # we return what's left (-> possibly smaller batch!) and prepare\n",
    "            # the start of a new epoch\n",
    "            batch = (train_data[current_pos:size],\n",
    "                     train_labels[current_pos:size])\n",
    "            current_pos = 0\n",
    "            print(\"Starting new epoch...\")\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'D_W1_2:0' shape=(784, 128) dtype=float32_ref>\", \"<tf.Variable 'D_W2_2:0' shape=(128, 1) dtype=float32_ref>\", \"<tf.Variable 'D_b1_2:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'D_b2_2:0' shape=(1,) dtype=float32_ref>\"] and loss Tensor(\"Neg_2:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\plugg\\Advanced Deep Learning\\D7047E\\Exercise5\\Exercise5.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/plugg/Advanced%20Deep%20Learning/D7047E/Exercise5/Exercise5.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39m# Only update D(X)'s parameters, so var_list = theta_D\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Downloads/plugg/Advanced%20Deep%20Learning/D7047E/Exercise5/Exercise5.ipynb#ch0000008?line=1'>2</a>\u001b[0m D_solver \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mAdamOptimizer()\u001b[39m.\u001b[39;49mminimize(D_loss, var_list\u001b[39m=\u001b[39;49mtheta_D)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/plugg/Advanced%20Deep%20Learning/D7047E/Exercise5/Exercise5.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39m# Only update G(X)'s parameters, so var_list = theta_G\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Downloads/plugg/Advanced%20Deep%20Learning/D7047E/Exercise5/Exercise5.ipynb#ch0000008?line=3'>4</a>\u001b[0m G_solver \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mAdamOptimizer()\u001b[39m.\u001b[39mminimize(G_loss, var_list\u001b[39m=\u001b[39mtheta_G)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py:415\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=412'>413</a>\u001b[0m vars_with_grad \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m g, v \u001b[39min\u001b[39;00m grads_and_vars \u001b[39mif\u001b[39;00m g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=413'>414</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vars_with_grad:\n\u001b[1;32m--> <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=414'>415</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=415'>416</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable, check your graph for ops\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=416'>417</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m that do not support gradients, between variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and loss \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=417'>418</a>\u001b[0m       ([\u001b[39mstr\u001b[39m(v) \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars], loss))\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=419'>420</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, global_step\u001b[39m=\u001b[39mglobal_step,\n\u001b[0;32m    <a href='file:///c%3A/Users/roman/AppData/Local/Programs/Python/Python39/lib/site-packages/tensorflow/python/training/optimizer.py?line=420'>421</a>\u001b[0m                             name\u001b[39m=\u001b[39mname)\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'D_W1_2:0' shape=(784, 128) dtype=float32_ref>\", \"<tf.Variable 'D_W2_2:0' shape=(128, 1) dtype=float32_ref>\", \"<tf.Variable 'D_b1_2:0' shape=(128,) dtype=float32_ref>\", \"<tf.Variable 'D_b2_2:0' shape=(1,) dtype=float32_ref>\"] and loss Tensor(\"Neg_2:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "# Only update D(X)'s parameters, so var_list = theta_D\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "# Only update G(X)'s parameters, so var_list = theta_G\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    '''Uniform prior for G(Z)'''\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "i = 0\n",
    "train = mnist[\"train\"]\n",
    "\n",
    "test = mnist[\"test\"]\n",
    "for it in range(1000000):\n",
    "    print(\"AAAAAA\")\n",
    "    print(train[0])\n",
    "    #X_mb = next_batch(i, mb_size, )\n",
    "    #i = i+128\n",
    "    \n",
    "\n",
    "    #_, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    #_, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca7165b3874c121c9f63e9490792d5288fe8335c417634071cb2f60b95a24850"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
